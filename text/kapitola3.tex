\chapter{Towards Building a Squiggle Index}

\label{kap:methAdjust}

In this chapter, we use the results and findings from the previous chapter to
build the index data structure that can find the query squiggle in the simulated
reference squiggle.

\section{Building a Squiggle Index}
\label{section:indexIdea}

As mentioned in Section \ref{section:selectiveSequencing}, as an input to the selective
sequencing we are usually only provided the reference DNA sequence.
The reference DNA sequence is transformed to a simulated signal using the
process introduced in Section \ref{section:simulateSquiggle}. We choose the number of the windows $w$
as a parameter of our discretization algorithm and create the reference level string.
Then, when the real signal from the squiggle arrives, we perform the same process
on the real signal. Now, we have one reference level string and
one query level string which we want to find in the reference.
As we see in Section \ref{section:results}, there is only a small probability that the level string
of the longer squiggle will match perfectly to some area in the reference level string.
Instead of looking for the exact match in the reference, we will cut the reference
level string into overlapping subsequences of length $k$ and put them into a hash table.
Hash table is a data structure that allows insertion and search of an element in
amortized time complexity $O(1)$. This hash table will now serve as a reference index.
To process a squiggle, we build its level string and cut it into the overlapping
subsequences of the same length $k$. As the next step, we compute how many of these subsequences can be found in the index.
We call all the subsequences of length $k$ from our query squiggle that are
present in the prepared hash table \textit{hits}. Our initial assumption is that the number
of hits will be considerably higher for the squiggle that belongs to the reference.

To evaluate this idea experimentally we have chosen one \textit{contig} (contig3) from the
\textit{sapIng} reference sequence.
Contig is a standalone DNA sequence, most of the time representing a single chromosome
of the organism. It is smaller than the whole reference which consists typically of multiple contigs.
We will build the index over the signal simulated from this reference sequence. We will predict,
based on the number of hits in our index if the read is from the reference contig
or not. We build the index in the slightly different way than we described previously.
At first, we will simulate the signal from the whole contig creating one, very long simulated contig squiggle.
Then we use the sliding window of size $5\,000$ with the window step equal to $3\,000$.
For every of this small windows, we will normalize the signal that is contained in it, smooth
it using the techniques we described and then create the level string from this signal.
We will cut this smaller level string into overlapping $k$-mers that we insert into the
hash table. We will be working with the $2\,000$ readouts from the query squiggles. If this
query squiggle matches simulated signal somewhere, it will be covered by one of our windows.

Beside the reference we have two sets of reads, one containing reads from the sapIng
dataset and the other one containing reads from the \textit{sapFun} dataset. Both of these datasets
are described in Section \ref{section:data}. We call the reads from the first set the
positive reads and the other negative reads. We use 190 reads from the both datasets.
From the whole sapIng dataset, we only choose the reads that aligned to the contig3
and also have at least 95\% of their length aligned. We also make sure
that the reads from the sapFun dataset have 0 alignments of at least 90\% of their length. 
We are interested in finding if our index can work as a good predictor. It will be
deciding if the squiggle is from the reference based only on the number of hits.
Figure \ref{obr:roc_index} shows what is the ROC curve of our index as a classificator.

\begin{figure}
\centerline{\includegraphics[width=1.0\textwidth, height=0.3\textheight]{images/roc_index}}
\caption[TODO]{ROC curve describing how good predictor is our index. Its decisions
are based on the number of hits of the respective squiggle}
\label{obr:roc_index}
\end{figure}

We see that our index is not performing very well. After looking at our index, we can
identify one of the problems. We see that some of the $k$-mers have very big number of occurences in the indexed
reference level string. We suspect that these $k$-mers are not that informative
because they are very common and they carry more information about the specific
properties of the level string rather than information about the underlying signal.
Figure \ref{obr:kmerCoverage} shows what part of our reference is covered by the $k$-mers
up to a certain frequency. We can see that a lot of $k$-mers are in our reference
level string more than $2\,000$ times.

\begin{figure}
\centerline{\includegraphics[width=1.0\textwidth, height=0.3\textheight]{images/kmerCoverage}}
\caption[TODO]{On $y$ axis is the percentage of the reference covered by the $k$-mers up to specific frequency
denoted on the $x$-axis.}
\label{obr:kmerCoverage}
\end{figure}

We will use the data from the Figure \ref{obr:kmerCoverage} to try to remove the
most frequent $k$-mers. We will remove that part of the most frequent $k$-mers such that
at least 50\% of our reference is still covered by the remaining reads. We also
call this process \textit{cut-off}.

\begin{figure}
\centerline{\includegraphics[width=1.0\textwidth, height=0.3\textheight]{images/cut_off_50}}
\caption[TODO]{ROC curve describing how good predictor our index is after the 50\% cut-off. We can see the information
for the numerous combinations of levels and kmer lengths.}
\label{obr:cut_off_50}
\end{figure}

We see in Figure \ref{obr:cut_off_50} that this did not helped at all. This can be because with the most frequent
$k$-mers we also removed a lot of so called fake hits. This could be hits that amounted to the big
percentage of the previous hits so the overall number of the hits in our reference decreased
because of this fact.

\section{Aligning the squiggle level string}
\label{section:alignMinimap}

After failing to build the index straightly, we will look at an easier experiment
that can hint us if we are going in the good direction. We would want to see how
our discretization suits this usage and how similar is level string of the whole squiggle
comparing it to the whole reference level string. For this purpose, we will try to
find our query level string in the reference level string.

We already know that it does not make sense to look for the exact match of the query string in the reference
so we will need some string searching technique that can deal with few errors.
Our alignment algorithm presented in Section \ref{section:alignment} could be adapted but it lacks the neccessary speed. This
algorithm works in the time complexity $O(r\cdot s)$ where $r$ is the length of the
reference sequence and $q$ is the length of the query string. Many algorithms are able
to do this fast and with very high accuracy. We decided to tweak a Minimap2 \cite{li2018minimap2} algorithm. This is
one of the most popular DNA sequence aligning algorithms. This algorithm can take the
query DNA sequence and find it in a long reference DNA sequence. It is also able to
find the query DNA string even if it does not exactly match any subsequence precisely.
Hovewer, this algorithm works only with the DNA sequences. What we will do is that we choose
the number of levels $w=4$. This will cause that the reference and query level strings will both consist of
the characters 'a', 'b', 'c', 'd'. We then substitute 'a', 'b', 'c', 'd' by
'A', 'C', 'G', 'T' subsequently. We now obtained the manipulated level strings that
are represented using the DNA sequences. Now, we can use the Minimap2 algorithm for
finding our manipulated query level string in the manipulated reference level string.
As the Minimap2 is most of the time used and also optimized to perform well on DNA
sequences, we need to be careful of some catches that come with using it for this
purpose. For example, we need to ignore the hits on the reverse strand as the
reverse strand does not carry the same information as in the real DNA sequence.
This also speeds up the whole algorithm considerably.

We now take the level string of the relatively big part of the squiggle. We take the
fixed part of the squiggle from the $5\,000$th readout to the $60\,000$th. Of the 50
tested squiggles, all of them were correctly aligned to the correct contig and also
correct position. This was a very promising result, so we decided to optimize a
Minimap2 algorithm with adjusting some hyperparameters that this algorithm allows us
to change to speed it up. For this solution, to be usable, we need to take only
a shorter part from the start of the squiggle. With signal from the $5\,000$th
readout to the $10\,000$th we have been able to find only the 31 out of 100 squiggles
in around 1 minute.

\section{Alignment Algorithms Inspiration}
\label{section:alignmentAlgorithms}

The second approach that we tried is similar to how the alignment algorithms like
Minimap2 work. The first step of these algorithms is to find the pairs of the shorter
exactly matching sequences. These are then used as the kernels of the alignment. We want
to use kernels as the starting points, to see where are the places that could lead to the
successful matching of the query level strings.

In order to try this approach we need to know what is the ideal length of this short
sequence. We need to find the maximum length of the $k$-mer for the particular level
such that the level strings of most of the squiggles will have at least one $k$-mer
of this length common with their corresponding simulated reference level string.
As an experiment, we will take 200 random reads from the sapIng dataset.
We run the experiment on the data used in the previous experiments in Section
\ref{section:results}. Now, we will again work with the smaller number of $2\,000$ readouts so we
simulate the conditions during the real DNA sequencing. In the table \ref{tab:sharedKmers}
we can see the number of the squiggles with at least one shared $k$-mer with their reference
level string. We bring the values for the multiple levels. Based on this table,
we can expect what length of $k$-mer for the particular number of levels will be located
in the reference at least one time. We want to find the good tradeof between number of
squiggles that we will be able to find and the length of the $k$-mer which determines
the number of hits in the index we have to investigate.

\begin{table}
\caption[TODO]{The number of squiggles with at least one shared $k$-mer with their
corresponding reference for particular number of levels. The total number of tested
reads is 200}
%id tabulky
\label{tab:sharedKmers}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
level number & \multicolumn{10}{|c|}{$k$-mer lengths} \\
\hline
4 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 \\\cline{2-11}
& 200 & 199 & 198 & 196 & 195 & 186 & 180 & 174 & 163 & 148 \\\cline{2-11}
\hline
5 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 \\\cline{2-11}
& 200 & 199 & 199 & 197 & 190 & 184 & 176 & 163 & 158 & 148 \\\cline{2-11}
\hline
7 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 \\\cline{2-11}
& 200 & 199 & 198 & 195 & 183 & 167 & 157 & 132 & 119 & 100 \\\cline{2-11}
\hline
9 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 \\\cline{2-11}
& 200 & 199 & 191 & 184 & 160 & 134 & 108 & 86 & 71 & 49 \\\cline{2-11}
\hline
11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 \\\cline{2-11}
& 200 & 199 & 192 & 175 & 153 & 124 & 95 & 72 & 54 & 41 \\\cline{2-11}
\hline
13 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\\cline{2-11}
& 200 & 196 & 190 & 176 & 152 & 122 & 82 & 63 & 44 & 28 \\\cline{2-11}
\hline
\end{tabular}
\end{center}
\end{table}

Now, when we have for all the levels the particular number of $k$ that
has the minimal number of hits for most of the reads we can proceed to use
this information. We will focus on finding the pairs of level number - $k$-mer length
such that the number of squiggles with at least one hit is around $175$. This
will not eliminate a lot of reads and it will give us an edge over the fake hits in the
big reference sequence as we favor the biggest $k$-mer length possible.
We now take the entire reference of the sapIng dataset and $200$ reads from this
reference. We will want to know, what is the mean and median number of hits of
these squiggles in the entire reference and what is the number of the hits between
the squiggle and its corresponding simulated reference signal. We need to push the number of $k$-mers
for the particular number of levels as high as possible such that the large percentage
of reads still share at least some $k$-mers with their simulated squiggle but the
overal number of hits in the whole reference is smallest possible. What happens
if we do not push the $k$-mer length enough is that we receive a lot of false hits in
the entire reference. If we push too much, we will lose even the good hits and the
results will be some random hits in the entire reference that represent very similar
signal but not our target signal.

\begin{table}
\caption[TODO]{The number of hits in the whole reference vs hits in the simulated
squiggle corresponding to the real squiggle}
\label{tab:hitsRefvsSimul}
\begin{center}
\begin{tabular}{|l|l|c|c|c|c|}
\hline
levels & $k$-mer & \specialcell{mean hits\\in ref} & \specialcell{median hits\\in ref} & \specialcell{mean hits\\in simulated} & \specialcell{median hits\\in simulated} \\
\hline
4 & 22 & 223446.62 & 193751.0 & 16.28 & 12.0 \\
\hline
4 & 23 & 153540.78 & 132338.0 & 13.1 & 8.0 \\
\hline
5 & 21 & 401133.12 & 363887.5 & 23.32 & 19.5 \\
\hline
5 & 22 & 275380.16 & 241922.5 & 18.26 & 14.5 \\
\hline
5 & 23 & 192186.58 & 158792.5 & 14.54 & 9.0 \\
\hline
7 & 20 & 271556.42 & 265182.0 & 17.06 & 14.5 \\
\hline
7 & 21 & 175926.02 & 169163.0 & 12.88 & 10.5 \\
\hline
9 & 18 & 213360.74 & 191186.0 & 12.54 & 10.5 \\
\hline
9 & 19 & 127401.82 & 107961.5 & 8.58 & 7.0 \\
\hline
9 & 20 & 76321.34 & 58724.0 & 5.96 & 5.0 \\
\hline
11 & 16 & 243431.82 & 231441.5 & 11.16 & 8.5 \\
\hline
11 & 17 & 132439.22 & 126377.0 & 6.56 & 4.0 \\
\hline
11 & 18 & 72565.3 & 63293.0 & 3.78 & 2.0 \\
\hline
13 & 14 & 371994.38 & 326821.0 & 16.14 & 14.0 \\
\hline
13 & 15 & 194185.4 & 157827.5 & 10.18 & 8.0 \\
\hline
\end{tabular}
\end{center}
\end{table}

We can see in Table \ref{tab:hitsRefvsSimul} that the number of hits in the whole reference is still really high.
What we can do is that we cut the reference into the smaller sequences and track the number
of hits in these individual sequences. We can focus on the sequences with the high number
of hits and then do there some post-processing to remove as many hits as possible.
